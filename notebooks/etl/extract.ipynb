{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split up into separate functions\n",
    "\n",
    "# TODO: Uncomment when the function is transfered to the source file\n",
    "# # Make sure the directory 'etl' exists in the logs directory. If the directory does not exist, create it\n",
    "# if not os.path.exists('../logs/etl'):\n",
    "#     os.makedirs('../logs/etl')\n",
    "\n",
    "# Set up the logging configuration, for the ETL load process\n",
    "logging.basicConfig(\n",
    "    # Set the logging level to INFO, only messages with a level of INFO or higher will be displayed\n",
    "    level=logging.INFO,\n",
    "    # Set the format of the log messages, the format will be: 'timestamp - logger name - log level - log message'\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "\n",
    "    # TODO: Set to relative path\n",
    "    # Set the filename of the log file\n",
    "    filename='/Users/akram/DataScienceProjects/customer-churn-prediction/logs/etl/extract.log',\n",
    "    # 'w' mode will overwrite the log file every time the script is run, 'a' mode will append the log messages to the log file\n",
    "    filemode='w'\n",
    ")\n",
    "\n",
    "def extract_data(source_file_path, config_file_path):\n",
    "    \"\"\"\n",
    "    Extracts the data from a source file, verifies if the file is in csv format, and checks if it contains the expected columns. \n",
    "    Otherwise it exits with an exception.\n",
    "\n",
    "    @param \n",
    "    - file_path (str): The path to the file to extract\n",
    "    -config_file_path (str): The path to the configuration file, which contains the required columns\n",
    "    \n",
    "    @return pd.DataFrame: The data extracted from the file in a dataframe\n",
    "\n",
    "    @exceptions:\n",
    "    - ValueError: If the file is not a csv file\n",
    "    - FileNotFoundError: If the file is not found\n",
    "    - FileNotFoundError: If the configuration file is not found\n",
    "    - ValueError: If the data is missing required columns\n",
    "    \"\"\"\n",
    "    # Log a message that the extract process has started\n",
    "    logging.info(f\"Starting data extract process for: {source_file_path}\")\n",
    "\n",
    "    # Check if the file is a csv file, if not a csv file, set an error message and raise a ValueError\n",
    "    if not source_file_path.endswith('.csv'):\n",
    "        # Log an error message\n",
    "        logging.error(f\"Invalid file type for file: {source_file_path}. Expected a csv file.\")\n",
    "        # Raise a ValueError\n",
    "        raise ValueError(f\"Invalid file type for file: {source_file_path}. Expected a csv file.\")\n",
    "        \n",
    "    else:\n",
    "        # Try to read the file\n",
    "        try:\n",
    "            # Load the data into a pandas DataFrame\n",
    "            data = pd.read_csv(source_file_path)\n",
    "            # Log a message that the data was successfully loaded\n",
    "            logging.info(f\"Data succesfully extracted from: {source_file_path}\")\n",
    "\n",
    "        # If the file is not found, catch the FileNotFoundError exception\n",
    "        except FileNotFoundError:\n",
    "            # Log an error message\n",
    "            logging.error(f\"File not found: {source_file_path}\")\n",
    "            # Raise a FileNotFoundError\n",
    "            raise FileNotFoundError(f\"File not found: {source_file_path}\")\n",
    "            \n",
    "    # Load the YAML configuration, which contains the required columns\n",
    "    # Try to open the configuration file\n",
    "    try:\n",
    "        # Open the configuration file, in read mode, the path is hardcoded since the same configuration file is used for all ETL processes\n",
    "        with open(config_file_path, 'r') as file:\n",
    "            # Load the YAML file, and store it in a dictionary\n",
    "            etl_extract_required_columns = yaml.safe_load(file)\n",
    "        # Access required columns\n",
    "        required_columns = etl_extract_required_columns['columns']['required_columns_to_load']\n",
    "        # Log a message that the required columns were successfully loaded\n",
    "        logging.info(f'Required columns succesfully extracted from configuration file.')\n",
    "    \n",
    "    # If the configuration file is not found, catch the FileNotFoundError exception\n",
    "    except FileNotFoundError:\n",
    "        # Log an error message\n",
    "        logging.error(f\"Configuration file not found: {config_file_path}\")\n",
    "        # Raise a FileNotFoundError\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {config_file_path}\")\n",
    "\n",
    "    # Validate if the data contains all the required \n",
    "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "    if missing_columns:\n",
    "        logging.error(f\"The file is missing required columns: {missing_columns}\")\n",
    "        raise ValueError(f\"Schema validation failed. Missing columns: {missing_columns}\")\n",
    "\n",
    "    # Log a successful schema validation\n",
    "    logging.info(\"Schema validation passed. All required columns are present.\")\n",
    "\n",
    "    # Return the data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_data('/Users/akram/DataScienceProjects/customer-churn-prediction/data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv', \n",
    "                    '/Users/akram/DataScienceProjects/customer-churn-prediction/configuration/etl/extract/required_columns.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customerchurn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
