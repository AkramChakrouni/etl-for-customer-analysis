{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import yaml\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_section):\n",
    "    \"\"\"\n",
    "    Set up the logging configuration dynamically for various ETL processes\n",
    "\n",
    "    @param log_section: str, the section of the logging configuration to use\n",
    "    \"\"\"\n",
    "    config_path = '/Users/akram/DataScienceProjects/customer-churn-prediction/configuration/etl/logging_paths.yaml'\n",
    "    with open(config_path, 'r') as file:\n",
    "        log_config = yaml.safe_load(file)\n",
    "\n",
    "    path = log_config['etl'][log_section]\n",
    "    log_dir = os.path.dirname(path)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        filename=path,\n",
    "        filemode='w'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data_file(file_path):\n",
    "    \"\"\"\n",
    "    Open a file and return pandas DataFrame\n",
    "\n",
    "    @param file_path (str): The path to the file to open\n",
    "    @return pd.DataFrame: The data extracted from the file in a dataframe\n",
    "    \"\"\"\n",
    "    # Log a message that the file is being opened\n",
    "    logging.info(f\"Opening file: {file_path}\")\n",
    "\n",
    "    # Check if the file is a csv file, if not a csv file, set an error message and raise a ValueError\n",
    "    if not file_path.endswith('.csv'):\n",
    "        # Log an error message\n",
    "        logging.error(f\"Invalid file type for file: {file_path}. Expected a csv file.\")\n",
    "        # Raise a ValueError\n",
    "        raise ValueError(f\"Invalid file type for file: {file_path}. Expected a csv file.\")\n",
    "        \n",
    "    else:\n",
    "        # Try to read the file\n",
    "        try:\n",
    "            # Load the data into a pandas DataFrame\n",
    "            data = pd.read_csv(file_path)\n",
    "            # Log a message that the data was successfully loaded\n",
    "            logging.info(f\"Data succesfully extracted from: {file_path}\")\n",
    "\n",
    "        # If the file is not found, catch the FileNotFoundError exception\n",
    "        except FileNotFoundError:\n",
    "            # Log an error message\n",
    "            logging.error(f\"File not found: {file_path}\")\n",
    "            # Raise a FileNotFoundError\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "    # Return the file object\n",
    "    return data\n",
    "\n",
    "def open_config_file(file_path):\n",
    "    \"\"\"\n",
    "    Open a configuration file and return a dictionary\n",
    "\n",
    "    @param file_path (str): The path to the configuration file to open\n",
    "    @return dict: The configuration data extracted from the file\n",
    "    \"\"\"\n",
    "    # Log a message that the configuration file is being opened\n",
    "    logging.info(f\"Opening configuration file: {file_path}\")\n",
    "\n",
    "    # Load the YAML configuration, which contains the required columns\n",
    "    # Try to open the configuration file\n",
    "    try:\n",
    "        # Open the configuration file, in read mode, the path is hardcoded since the same configuration file is used for all ETL processes\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Load the YAML file, and store it in a dictionary\n",
    "            config_col = yaml.safe_load(file)\n",
    "        # Log a message that the required columns were successfully loaded\n",
    "        logging.info(f'Required columns succesfully extracted from configuration file.')\n",
    "    \n",
    "    # If the configuration file is not found, catch the FileNotFoundError exception\n",
    "    except FileNotFoundError:\n",
    "        # Log an error message\n",
    "        logging.error(f\"Configuration file not found: {file_path}\")\n",
    "        # Raise a FileNotFoundError\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {file_path}\")\n",
    "    \n",
    "    # Return the configuration data\n",
    "    return config_col\n",
    "\n",
    "def schema_validation(data, required_columns):\n",
    "    \"\"\"\n",
    "    Validate if the data contains the required columns\n",
    "\n",
    "    @param data (pd.DataFrame): The data extracted from the file in a dataframe\n",
    "    @param required_columns (list): The list of required columns to validate\n",
    "    \"\"\"\n",
    "    # Validate if the data contains all the required \n",
    "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "    if missing_columns:\n",
    "        logging.error(f\"The file is missing required columns: {missing_columns}\")\n",
    "        raise ValueError(f\"Schema validation failed. Missing columns: {missing_columns}\")\n",
    "\n",
    "    # Log a successful schema validation\n",
    "    logging.info(\"Schema validation passed. All required columns are present.\")\n",
    "\n",
    "def extract_data(data_file_path, config_file_path):\n",
    "    \"\"\"\n",
    "    Extracts the data from a source file, verifies if the file is in csv format, and checks if it contains the expected columns. \n",
    "    Otherwise it exits with an exception.\n",
    "\n",
    "    @param \n",
    "    - file_path (str): The path to the file to extract\n",
    "    -config_file_path (str): The path to the configuration file, which contains the required columns\n",
    "    \n",
    "    @return pd.DataFrame: The data extracted from the file in a dataframe\n",
    "\n",
    "    @exceptions:\n",
    "    - ValueError: If the file is not a csv file\n",
    "    - FileNotFoundError: If the file is not found\n",
    "    - FileNotFoundError: If the configuration file is not found\n",
    "    - ValueError: If the data is missing required columns\n",
    "    \"\"\"\n",
    "    # Set up the logging configuration, for the ETL extract process\n",
    "    setup_logging('extract')\n",
    "\n",
    "    # Log a message that the extract process has started\n",
    "    logging.info(f\"Starting data extract process for: {data_file_path}\")\n",
    "\n",
    "    # Open the file\n",
    "    data = open_data_file(data_file_path)\n",
    "            \n",
    "    # Open the configuration file, which contains the required columns\n",
    "    config_col = open_config_file(config_file_path)\n",
    "\n",
    "    # Access required columns\n",
    "    required_columns = config_col['columns']['required_columns_to_load']\n",
    "    \n",
    "    # Validate if the data contains all the required \n",
    "    schema_validation(data, required_columns)\n",
    "\n",
    "    # Return the data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, columns_to_normalize):\n",
    "    \"\"\"\n",
    "    Normalizes the columns in the data, using the MinMaxScaler\n",
    "    Normalization is chosen over standardization because the data's difference is meaningful \n",
    "    and the data is not normally distributed. \n",
    "\n",
    "    @param data (pd.DataFrame): The data to normalize\n",
    "    @param columns_to_normalize (list): The columns to normalize\n",
    "    @return pd.DataFrame: The data with the normalized columns\n",
    "    \"\"\"\n",
    "    # Log a message that the normalization process has started\n",
    "    logging.info(\"Starting normalization process\")\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # Normalize the columns\n",
    "    data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])\n",
    "\n",
    "    # Log a message that the normalization process has ended\n",
    "    logging.info(\"Normalization process completed\")\n",
    "\n",
    "    # Return the data with the normalized columns\n",
    "    return data\n",
    "\n",
    "def label_encode(data, columns_to_label_encode):\n",
    "    \"\"\"\n",
    "    Label encodes the columns in the data, using the LabelEncoder.\n",
    "    Label encoding is used for columns that are ordinal, meaning the data has a meaningful order.\n",
    "\n",
    "    @param data (pd.DataFrame): The data to label encode\n",
    "    @param columns_to_label_encode (list): The columns to label encode\n",
    "    @return pd.DataFrame: The data with the label encoded columns\n",
    "    \"\"\"\n",
    "    # Log a message that the label encoding process has started\n",
    "    logging.info(\"Starting label encoding process\")\n",
    "\n",
    "    # Initialize the LabelEncoder object\n",
    "    label_encoder = LabelEncoder()\n",
    "    # Label encode the columns\n",
    "    data[columns_to_label_encode] = data[columns_to_label_encode].apply(label_encoder.fit_transform)\n",
    "\n",
    "    # Log a message that the label encoding process has ended\n",
    "    logging.info(\"Label encoding process completed\")\n",
    "\n",
    "    # Return the data with the label encoded columns\n",
    "    return data\n",
    "\n",
    "def one_hot_encode(data, columns_to_one_hot_encode):\n",
    "    \"\"\"\n",
    "    One hot encodes the columns in the data, using the get_dummies method.\n",
    "    One hot encoding is used for columns that are nominal, meaning the data has no meaningful order.\n",
    "\n",
    "    @param data (pd.DataFrame): The data to one hot encode\n",
    "    @param columns_to_one_hot_encode (list): The columns to one hot encode\n",
    "    @return pd.DataFrame: The data with the one hot encoded columns\n",
    "    \"\"\"\n",
    "    # Log a message that the one hot encoding process has started\n",
    "    logging.info(\"Starting one hot encoding process\")\n",
    "\n",
    "    # One hot encode the columns\n",
    "    data = pd.get_dummies(data, columns=columns_to_one_hot_encode, dtype=int)\n",
    "\n",
    "    # Log a message that the one hot encoding process has ended\n",
    "    logging.info(\"One hot encoding process completed\")\n",
    "\n",
    "    # Return the data with the one hot encoded columns\n",
    "    return data\n",
    "\n",
    "def open_config_file(file_path):\n",
    "    \"\"\"\n",
    "    Open a configuration file and return a dictionary\n",
    "\n",
    "    @param file_path (str): The path to the configuration file to open\n",
    "    @return dict: The configuration data extracted from the file\n",
    "    \"\"\"\n",
    "    # Log a message that the configuration file is being opened\n",
    "    logging.info(f\"Opening configuration file: {file_path}\")\n",
    "    \n",
    "    # Load the YAML configuration, which contains the columns\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            config_col = yaml.safe_load(file)\n",
    "        # Log a message that the configuration file was loaded successfully\n",
    "        logging.info(\"Configuration file loaded successfully\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Configuration file not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {file_path}\")\n",
    "    \n",
    "    return config_col\n",
    "\n",
    "def transform(data, config_file):\n",
    "    \"\"\"\n",
    "    Transforms the data by normalizing, label encoding, and one hot encoding the columns in the data.\n",
    "    The columns to normalize, label encode, and one hot encode are loaded from the configuration file.\n",
    "\n",
    "    @param data (pd.DataFrame): The data to transform\n",
    "    @param config_file (str): The path to the configuration file\n",
    "    @return pd.DataFrame: The transformed data\n",
    "    \"\"\"\n",
    "    # Set up the logging configuration, for the ETL transform process\n",
    "    setup_logging('transform')\n",
    "\n",
    "    # Log a message that the transform process has started\n",
    "    logging.info(\"Starting data transform process\")\n",
    "\n",
    "    # Open the configuration file, which contains the columns\n",
    "    config_columns = open_config_file(config_file)\n",
    "    \n",
    "    # Access the columns from the configuration file\n",
    "    columns_to_change_from_object_to_numeric = config_columns['columns']['columns_to_change_from_object_to_numeric']\n",
    "    columns_one_hot_encode = config_columns['columns']['columns_one_hot_encode']\n",
    "    columns_label_encode = config_columns['columns']['columns_label_encode']\n",
    "    columns_to_normalize = config_columns['columns']['columns_to_normalize']\n",
    "    # Columns that depend on another column, these columns will be label encoded\n",
    "    columns_that_depend_on_another_column = config_columns['columns']['columns_that_depend_on_another_column']\n",
    "\n",
    "    # Change the columns from object to numeric\n",
    "    data[columns_to_change_from_object_to_numeric] = data[columns_to_change_from_object_to_numeric].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fill missing values with zero value for the columns that will be converted to numeric, since empty means no information about the customer, which means no charges has been made\n",
    "    data[columns_to_change_from_object_to_numeric] = data[columns_to_change_from_object_to_numeric].fillna(0)\n",
    "\n",
    "    # Normalize the columns\n",
    "    data = normalize(data, columns_to_normalize)\n",
    "\n",
    "    # Label encode independent columns\n",
    "    data = label_encode(data, columns_label_encode)\n",
    "\n",
    "    # One hot encode the columns\n",
    "    data = one_hot_encode(data, columns_one_hot_encode)\n",
    "\n",
    "    # Label encode columns that depend on another column\n",
    "    data = label_encode(data, columns_that_depend_on_another_column)\n",
    "    \n",
    "    # Log a message that the transform process has ended\n",
    "    logging.info(\"Data transform process completed\")\n",
    "\n",
    "    # Return the transformed data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, db_path):\n",
    "    \"\"\"\n",
    "    Load the transformed data into a SQLite database\n",
    "\n",
    "    @param data (pd.DataFrame): The transformed data to load\n",
    "    @param db_path (str): The path to the SQLite database\n",
    "    \"\"\"\n",
    "    # Set up the logging configuration, for the ETL load process\n",
    "    setup_logging('load')\n",
    "\n",
    "    # Log a message that the load process has started\n",
    "    logging.info(\"Starting data load process\")\n",
    "\n",
    "    try:\n",
    "        # Connect to SQLite database (this will create a new file if it doesn't exist)\n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Create a table in SQLite dynamically from the DataFrame columns if it doesn't exist\n",
    "        # Using `to_sql` automatically handles table creation if it doesn't exist.\n",
    "        data.to_sql('customers', conn, if_exists='replace', index=False)\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        logging.info(f\"Data successfully loaded into database at {db_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data load: {e}\")\n",
    "    finally:\n",
    "        # Close the database connection\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl(data_file_path, config_file_path, db_path):\n",
    "    \"\"\"\n",
    "    Extracts, transforms, and loads the data from a source file to a SQLite database\n",
    "\n",
    "    @param data_file_path (str): The path to the file to extract\n",
    "    @param config_file_path (str): The path to the configuration file\n",
    "    @param db_path (str): The path to the SQLite database\n",
    "    \"\"\"\n",
    "    # Set up the logging configuration, for the ETL process\n",
    "    setup_logging('etl')\n",
    "\n",
    "    # Log a message that the ETL process has started\n",
    "    logging.info(\"Starting ETL process\")\n",
    "\n",
    "    # Extract the data\n",
    "    data = extract_data(data_file_path, config_file_path)\n",
    "\n",
    "    # Transform the data\n",
    "    transformed_data = transform(data, config_file_path)\n",
    "\n",
    "    # Load the transformed data into the SQLite database\n",
    "    load_data(transformed_data, db_path)\n",
    "\n",
    "    # Log a message that the ETL process has ended\n",
    "    logging.info(\"ETL process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl('/Users/akram/DataScienceProjects/customer-churn-prediction/data/WA_Fn-UseC_-Telco-Customer-Churn.csv', \n",
    "                    '/Users/akram/DataScienceProjects/customer-churn-prediction/configuration/etl/data_columns.yaml', \n",
    "                    '/Users/akram/DataScienceProjects/customer-churn-prediction/data_warehouse/customers.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>...</th>\n",
       "      <th>Churn</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>InternetService_DSL</th>\n",
       "      <th>InternetService_Fiber optic</th>\n",
       "      <th>InternetService_No</th>\n",
       "      <th>PaymentMethod_Bank transfer (automatic)</th>\n",
       "      <th>PaymentMethod_Credit card (automatic)</th>\n",
       "      <th>PaymentMethod_Electronic check</th>\n",
       "      <th>PaymentMethod_Mailed check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  SeniorCitizen  Partner  Dependents    tenure  PhoneService  \\\n",
       "0  7590-VHVEG              0        1           0  0.013889             0   \n",
       "1  5575-GNVDE              0        0           0  0.472222             1   \n",
       "2  3668-QPYBK              0        0           0  0.027778             1   \n",
       "3  7795-CFOCW              0        0           0  0.625000             0   \n",
       "4  9237-HQITU              0        0           0  0.027778             1   \n",
       "\n",
       "   MultipleLines  OnlineSecurity  OnlineBackup  DeviceProtection  ...  Churn  \\\n",
       "0              1               0             2                 0  ...      0   \n",
       "1              0               2             0                 2  ...      0   \n",
       "2              0               2             2                 0  ...      1   \n",
       "3              1               2             0                 2  ...      0   \n",
       "4              0               0             0                 0  ...      1   \n",
       "\n",
       "   gender_Female  gender_Male  InternetService_DSL  \\\n",
       "0              1            0                    1   \n",
       "1              0            1                    1   \n",
       "2              0            1                    1   \n",
       "3              0            1                    1   \n",
       "4              1            0                    0   \n",
       "\n",
       "   InternetService_Fiber optic  InternetService_No  \\\n",
       "0                            0                   0   \n",
       "1                            0                   0   \n",
       "2                            0                   0   \n",
       "3                            0                   0   \n",
       "4                            1                   0   \n",
       "\n",
       "   PaymentMethod_Bank transfer (automatic)  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        1   \n",
       "4                                        0   \n",
       "\n",
       "   PaymentMethod_Credit card (automatic)  PaymentMethod_Electronic check  \\\n",
       "0                                      0                               1   \n",
       "1                                      0                               0   \n",
       "2                                      0                               0   \n",
       "3                                      0                               0   \n",
       "4                                      0                               1   \n",
       "\n",
       "   PaymentMethod_Mailed check  \n",
       "0                           0  \n",
       "1                           1  \n",
       "2                           1  \n",
       "3                           0  \n",
       "4                           0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('/Users/akram/DataScienceProjects/customer-churn-prediction/data_warehouse/customers.db')\n",
    "\n",
    "# Query to retrieve all the data from the 'customers' table\n",
    "query = \"SELECT * FROM customers\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customerchurn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
