# etl-for-customer-analysis

ETL Pipeline for Data Transformation and Loading

This project demonstrates an **Extract, Transform, Load (ETL)** pipeline built with Python. It extracts data from source files, transforms it for consistency and usability, and loads it into a SQLite database. This pipeline is modular and uses best practices for logging, configuration management, and error handling.

## ğŸš€ Features

- **Modular Architecture**: Separates extraction, transformation, and loading processes into individual modules.
- **Error Handling**: Validates schema and file formats with detailed logging and error messages.
- **Dynamic Configuration**: Uses YAML configuration files for flexible management of pipeline parameters.
- **Data Transformation**: Implements:
  - Normalization with `MinMaxScaler`.
  - Label encoding for ordinal data.
  - One-hot encoding for nominal data.
- **Database Integration**: Automatically creates and populates SQLite tables.

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.7+
- Required libraries: `pandas`, `scikit-learn`, `pyyaml`, `sqlite3`

## ğŸ“‚ Project Structure:
```
  .
  â”œâ”€â”€ configuration              # Configuration files for ETL and modeling
  â”‚   â”œâ”€â”€ etl                    # ETL-specific configurations
  â”‚   â”‚   â”œâ”€â”€ columns.yaml       # Column-specific transformation rules
  â”‚   â”‚   â””â”€â”€ logging_paths.yaml # Paths for log files
  â”‚   â””â”€â”€ model                  # Model-specific configurations
  â”‚       â””â”€â”€ model_config.yaml  # Configuration for model training
  â”œâ”€â”€ data                       # Data directory
  â”‚   â”œâ”€â”€ raw                    # Raw data files
  â”‚   â””â”€â”€ data_warehouse         # Processed data ready for analysis or modeling
  â”œâ”€â”€ logs                       # Log files
  â”‚   â”œâ”€â”€ etl                    # Logs related to ETL processes
  â”‚   â””â”€â”€ eda                    # Logs related to exploratory data analysis
  â”œâ”€â”€ notebooks                  # Jupyter notebooks for EDA and ETL
  â”‚   â”œâ”€â”€ etl                    # ETL-specific notebooks
  â”‚   â”‚   â””â”€â”€ etl_pipeline_demo.ipynb
  â”‚   â””â”€â”€ eda                    # EDA-specific notebooks
  â”‚       â””â”€â”€ eda_analysis.ipynb
  â”œâ”€â”€ src                        # Source code
  â”‚   â”œâ”€â”€ etl_pipeline           # ETL pipeline code
  â”‚   â”‚   â”œâ”€â”€ extract.py         # Handles data extraction and validation
  â”‚   â”‚   â”œâ”€â”€ transform.py       # Handles data transformations
  â”‚   â”‚   â”œâ”€â”€ load.py            # Handles data loading into SQLite database
  â”‚   â””â”€â”€ model                  # Model-related code
  â”‚       â”œâ”€â”€ train_model.py     # Training machine learning models
  â”‚       â””â”€â”€ evaluate_model.py  # Model evaluation scripts
  â”œâ”€â”€ README.md                  # Project documentation
  â”œâ”€â”€ requirements.txt           # Python dependencies
  â””â”€â”€ main.py                    # Entry point for running the ETL pipeline
```

## ğŸ”‘ Usage
##### Running the ETL Pipeline

To execute the ETL pipeline:
```
python etl.py --data_file_path <data-file> --config_file_path <config-file> --db_path <database-path>
```

##### Example:
```
python etl.py --data_file_path data/raw/customers.csv --config_file_path configuration/etl/columns.yaml --db_path data/data_warehouse/customers.db
```

## ETL Flow

	1.	Extract: Validates and loads CSV data into a Pandas DataFrame.
	2.	Transform:
	              â€¢	Converts data types.
	              â€¢	Handles missing values.
	              â€¢	Applies normalization, label encoding, and one-hot encoding.
	3.	Load: Inserts transformed data into a SQLite database.

## ğŸ“ Configuration

The pipeline uses YAML configuration files:
	â€¢	columns.yaml: Specifies columns for transformation (e.g., normalization, encoding).
	â€¢	logging_paths.yaml: Defines paths for log files for each process (e.g., extract.log, transform.log).

 
##### Example YAML file (columns.yaml):
```
columns:
  columns_to_change_from_object_to_numeric:
    - total_charges
    - monthly_charges
  columns_label_encode:
    - contract_type
  columns_one_hot_encode:
    - payment_method
  columns_to_normalize:
    - tenure
```

## ğŸ“ˆ Logs

Logs are automatically generated during the pipeline execution:
	â€¢	extract.log
	â€¢	transform.log
	â€¢	load.log

 ##### Example log snippet:
 ```
 2024-12-03 10:15:20,123 - extract - INFO - Data successfully extracted from: data/customers.csv
```

## ğŸ›¡ï¸ Error Handling

	â€¢	Missing Columns: Validates against required schema.
	â€¢	File Not Found: Gracefully handles missing files with descriptive errors.
	â€¢	Invalid File Format: Only supports .csv.



